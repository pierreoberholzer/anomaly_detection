{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this example we will take the AutoEncoder model trained in the notebook `Example Model Development.ipynb` and go through the steps which would be needed to deploy it into a production environment in the cloud using the command line tool `composer` which is designed to move a data science model from a development environment such as jupyterhub or on a laptop to a cloud tenant. The cloud tenant will have new data streaming in from remote locations so that we can use our model to make predictions on those data in real time.\n",
    "\n",
    "# Arundo Fabric\n",
    "\n",
    "For the workshop we have held back some of the data and are now streaming it into Arundo's industrial enterprise cloud environment Fabric. You can access Fabric by navigating to https://sand01.arundo.com and using the login credentials provided at the workshop (ask if you don't know the credentials!) Note that we recommend to use Chrome as your browser  although others should work too.\n",
    "\n",
    "When logged in you will see a screen like this:\n",
    "\n",
    "<img src=\"data/images/fabric.png\" width=85% />\n",
    "\n",
    "The key components you will use are the Models Manager and the Pipelines Manager. \n",
    "\n",
    "## Models Manager\n",
    "\n",
    "When you deploy the model its status will be visible in the Models Manager, you can also see recent logs for your model there in case it does not seem to be performing as you expect or the deployment fails. Feel free to look at the model which has already been deployed with name `AutoEncoder_drillxgit` by clicking on its name in the table:\n",
    "\n",
    "<img src=\"data/images/fabric_models.png\" width=85% />\n",
    "<img src=\"data/images/deployed_model.png\" width=85% />\n",
    "\n",
    "Don't worry about the `404` responses you see for `GET metrics/` that functionality has not been setup for this workshop so that's expected.\n",
    "\n",
    "Return to the model list by hitting the back button on your browser, now click on the `Use` box for that model. You will need to login again with the same credentials; this is because the model lives at a standalone url to the Fabric tenant but has been assigned the same user permissions so only users of the tenant can access it.\n",
    "\n",
    "When logged in you will see a screen like this:\n",
    "\n",
    "<img src=\"data/images/model_landing.png\" width=85% />\n",
    "\n",
    "If you navigate to the POST link for endpoint `stream_prediction` you will see an autorendered web form:\n",
    "\n",
    "<img src=\"data/images/endpoint1a.png\" style=\"display:inline\" width=48% />\n",
    "<img src=\"data/images/endpoint1b.png\" style=\"display:inline\" width=48% />\n",
    "\n",
    "This is not particularly useful as to make a prediction you have to enter all 29 input values by hand and hit `Execute Model`; however, the purpose of this endpoint is not to be interactive but to automatically make predictions on a stream of data. You will go through the steps of setting this up and will see what that looks like in the Pipelines Manager section below.\n",
    "\n",
    "If you navigate to the POST link for the endpoint `calculate_score` you will see a different autoredered web form. Leave the box blanks and hit `Execute Model`, this will generate a tab showing the current model score and a plot of the confusion matrix of the hidden data. Don't worry that the score is 0.0, it just used some toy data, we will give the SAS token for the real hidden data during the workshop; if you think you are ready to score your model please ask for the SAS token.\n",
    "\n",
    "<img src=\"data/images/endpoint2.png\" width=85% />\n",
    "\n",
    "## Pipelines Manager\n",
    "\n",
    "If you now navigate to the Pipelines Manager in Fabric you will see a page like this:\n",
    "\n",
    "<img src=\"data/images/pipeline.png\" width=85% />\n",
    "\n",
    "Click on the name of the example deployed pipeline `autoencoder_driixgit`, after it has loaded you will see the data streaming in on the left, one plot for each of the 29 sensor variables. On the right you will see the model prediction for the `stream_prediction` endpoint we looked at above which we have used `composer` to attach to the pipeline. Note that the version of Pipelines Manager we are using for this workshop only supports one model output.\n",
    "\n",
    "<img src=\"data/images/streaming_pipeline.png\" width=85% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arundo Composer\n",
    "\n",
    "We will now go through the steps needed to deploy a model to Fabric using the Composer command line tool. You might find it useful to open a terminal from the your notebook homepage, but we can also issue command line calls directly from this notebook using the `!` prefix. The environment you are working in already has Composer installed. First let's briefly explore the kind of functionality Composer offers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: composer [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  A CLI tool for Arundo Fabric\r\n",
      "\r\n",
      "Options:\r\n",
      "  --debug / --no-debug\r\n",
      "  --local / --no-local\r\n",
      "  --help                Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  bundle          Bundle up a model for publishing\r\n",
      "  check           See what your current settings are\r\n",
      "  clear_password  Clears a saved password\r\n",
      "  config          See your config file\r\n",
      "  container       Container management commands\r\n",
      "  dev             Arundo developer only commands\r\n",
      "  extensions      Extension management commands\r\n",
      "  file            File management commands\r\n",
      "  init            Create a new model skeleton\r\n",
      "  login           Login to Arundo Fabric\r\n",
      "  logout          Log out of Fabric\r\n",
      "  model           Model management commands\r\n",
      "  pipeline        How pipelines are managed\r\n",
      "  publish         Publish a model to Fabric\r\n",
      "  refresh         Ensure that your access token is refreshed\r\n",
      "  request         Make a curl-style request with auth headers\r\n",
      "  run             Run a model locally for testing\r\n",
      "  set             Set config properties\r\n",
      "  subscription    How subscriptions are managed\r\n",
      "  switch          Switches which set of servers you connect to\r\n",
      "  tag             How tags are managed\r\n",
      "  token           See your access token for debug\r\n",
      "  validate        Validate a model without publishing\r\n",
      "  version         See installed composer version\r\n"
     ]
    }
   ],
   "source": [
    "!composer --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Fabric\n",
    "\n",
    "There are a wide range of commands a data scientist can use to develop and deploy their models. Next we want to get composer connected to the cloud tenant, as the current settings are not yet connected. The credentials you have for this workshop are for a Fabric tenant in environment `sand01` so we need to connect Composer to that environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've switched to a new server (sand01) and will have to login again\n",
      "Your settings have been saved.\n",
      "Composer Version:   0.2.1\n",
      "Email:              \n",
      "Saved Password:     False\n",
      "Server:             https://sand01.arundo.com\n",
      "Access Remaining:   None\n",
      "Refresh Remaining:  None\n",
      "OS Version:         linux\n"
     ]
    }
   ],
   "source": [
    "!composer switch sand01\n",
    "!composer check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set the email to match the Fabric credentials, this is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!composer set email <MY_USER_EMAIL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to the cloud tenant and obtain the authorization to interact with it. If you are working with the terminal you can run `$ composer login` (make sure you are in the directory `work`) and enter the password for your user email. This command won't work from the notebook as it requires user input at the command line which jupyter doesn't support). If you prefer to stay in the notebook you can ask one of the workshop conveners to provide you with an authorization token which you can then use in the following command to authorize and connect to the tenant environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSet your token!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!composer set token <MY_AUTH_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be connected which you can confirm by seeing that the field `Access Remaining` is no longer `None` but has some finite time remaining. If that's not the case please ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composer Version:   0.2.1\r\n",
      "Email:              user_amld@drillx.com\r\n",
      "Saved Password:     False\r\n",
      "Server:             https://sand01.arundo.com\r\n",
      "Access Remaining:   5h59m\r\n",
      "Refresh Remaining:  None\r\n",
      "OS Version:         linux\r\n"
     ]
    }
   ],
   "source": [
    "!composer check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Deployment\n",
    "\n",
    "Now that we have composer connected to the Fabric tenant we need to construct an `app.py` using the Arundo Python Model Runtime libraries which provide a user friendly way to generate the web interface endpoints you saw above  - essentially all of the complicated stuff is taken care of in decorators!\n",
    "\n",
    "To deploy to the cloud you will need three files:\n",
    "```\n",
    "app.py\n",
    "config.yaml\n",
    "requirements.txt```\n",
    "In fact `requirements.txt` is not even necessary if you have no additional Python library dependencies beyond the standard libraries. The following cells show the corresponding contents of these files for the model which you have already seen deployed, executing a given cell will overwrite the corresponding file in your notebook directory due to the `%%writefile <FILENAME>` header, so you can use these cells to edit the files you will deploy with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.yaml\n",
    "\n",
    "This file names the model and provides some basic version control. Note that in the tenant no two models can have the same name and version number regardless of which user deployed it. If you try to deploy a model whose name/version already exists the build will fail. To make sure your name does not clash with other workshop users we suggest to add your github user handle in the model name, this will also make it easier to find in the Models Management table when you have deployed to Fabric. If your build fails, fix the code and bump the version numer when redeploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "model_name: AutoEncoder-drillxgit\n",
    "model_version: 0.0.1\n",
    "wrapper_version: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "\n",
    "Here you should add all the Python library requirements needed to execute your model so that they are installed correctly in the deployed environment. The main cause of failures in the deployment process tends to be a missing entry in this file, particularly of hidden dependencies. If your model has associated non-python binary dependencies which would need e.g. `apt-get install <PACKAGE>` then you will need an extra file `dependencies.apt` which lists these. For most workflows that shouldn't be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "# needed by the model\n",
    "Keras==2.1.3\n",
    "tensorflow==1.4.1\n",
    "h5py==2.7.1\n",
    "\n",
    "# needed by the endpoint functions\n",
    "numpy==1.13.1\n",
    "scikit-learn==0.19.1\n",
    "matplotlib==2.1.2\n",
    "seaborn==0.8.1\n",
    "azure-storage==0.36.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### app.py\n",
    "\n",
    "This file is where most of the work is done. Here there have been two endpoints implemented, the details of which are given in the docstrings for those endpoints. To update this functionality to support your own model you should implement the functions `row_prediction` and `vectorized_prediction` following the example shown for the trained autoencode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "\"\"\"\n",
    "To deploy your own serialized model for deployment to a stream of live data\n",
    "and to calculate the score of your model on the hidden data implement\n",
    "the functions:\n",
    "\n",
    "row_prediction which takes a list as input in the order:\n",
    "[Vol, S1, S2, ..., S27, S28];\n",
    "\n",
    "vectorized_prediction which takes a np.ndarry as input with features as\n",
    "columns in the same order and entries as rows.\n",
    "\"\"\"\n",
    "# these imports are needed for the endpoints which you do not need to change\n",
    "from runtime.framework import (endpoint, argument, returns, visualization)\n",
    "from runtime.schema import fields\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from amld_functions import (get_data, get_score, plot_confmat)\n",
    "\n",
    "# model dependent libraries\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# global variables can be loaded into memory at deployment to improve efficiency\n",
    "# be careful if using tensorflow globally\n",
    "g_scaler = joblib.load('scaler.model')\n",
    "g_threshold99 = 3.8257\n",
    "\n",
    "\n",
    "def row_prediction(row):\n",
    "    \"\"\"\n",
    "    This function should return the prediction of your model on a single row\n",
    "    of data.\n",
    "    Inputs:\n",
    "    param row: np.array shape (, 29) with feature order\n",
    "               [Vol, S1, S2, ..., S27, S28]\n",
    "    Returns:\n",
    "    <bool>: True for predicted anomaly, False otherwise\n",
    "    \"\"\"\n",
    "    # apply the globally loaded feature scaler\n",
    "    scaled = g_scaler.transform(np.atleast_2d(row))\n",
    "    # load the trained autoencoder model\n",
    "    ae_model = load_model('model.h5')\n",
    "    # get predicted autoencoder output for the scaled input row\n",
    "    predicted = ae_model.predict(scaled)\n",
    "    # tidy up the Keras backend for this call\n",
    "    K.clear_session()\n",
    "    # calculate the mean squared error for this row\n",
    "    mse = mean_squared_error(predicted.T, scaled.T)\n",
    "    # return True if the mse is over the 99% threshold for non-anomalous data\n",
    "    return (mse > g_threshold99)\n",
    "\n",
    "\n",
    "def vectorized_prediction(matrix):\n",
    "    \"\"\"\n",
    "    This function should return a vector of predictions of your model on an\n",
    "    input matrix of batch data with m examples.\n",
    "    Inputs:\n",
    "    param matrix: np.ndarray shape (m, 29) with feature order\n",
    "                  [Vol, S1, S2, ..., S27, S28]\n",
    "    Returns:\n",
    "    <np.ndarray>: shape (m, 1) with m_i True for predicted anomaly and False\n",
    "                  otherwise\n",
    "    \"\"\"\n",
    "    # apply the globally loaded feature scaler\n",
    "    scaled = g_scaler.transform(matrix)\n",
    "    # load the trained autoencoder model\n",
    "    ae_model = load_model('model.h5')\n",
    "    # get predicted autoencoder output for the scaled input batch data\n",
    "    predicted = ae_model.predict(scaled)\n",
    "    # calculate the mean squared error for all examples\n",
    "    mse = mean_squared_error(predicted.T, scaled.T, multioutput='raw_values')\n",
    "    # return vector with True if the mse is over the 99% threshold\n",
    "    return (mse > g_threshold99)\n",
    "\n",
    "\n",
    "###\n",
    "# You do not need to edit below this line but are encouraged to understand\n",
    "# the implementation of the endpoints, just ask if you want more information\n",
    "# about endpoint implementation and functionality!\n",
    "###\n",
    "\n",
    "\n",
    "@endpoint()\n",
    "@argument(\"Vol\", type=float, description=\"Value of input sensor Vol\")\n",
    "@argument(\"S1\", type=float, description=\"Value of input sensor S1\")\n",
    "@argument(\"S2\", type=float, description=\"Value of input sensor S2\")\n",
    "@argument(\"S3\", type=float, description=\"Value of input sensor S3\")\n",
    "@argument(\"S4\", type=float, description=\"Value of input sensor S4\")\n",
    "@argument(\"S5\", type=float, description=\"Value of input sensor S5\")\n",
    "@argument(\"S6\", type=float, description=\"Value of input sensor S6\")\n",
    "@argument(\"S7\", type=float, description=\"Value of input sensor S7\")\n",
    "@argument(\"S8\", type=float, description=\"Value of input sensor S8\")\n",
    "@argument(\"S9\", type=float, description=\"Value of input sensor S9\")\n",
    "@argument(\"S10\", type=float, description=\"Value of input sensor S10\")\n",
    "@argument(\"S11\", type=float, description=\"Value of input sensor S11\")\n",
    "@argument(\"S12\", type=float, description=\"Value of input sensor S12\")\n",
    "@argument(\"S13\", type=float, description=\"Value of input sensor S13\")\n",
    "@argument(\"S14\", type=float, description=\"Value of input sensor S14\")\n",
    "@argument(\"S15\", type=float, description=\"Value of input sensor S15\")\n",
    "@argument(\"S16\", type=float, description=\"Value of input sensor S16\")\n",
    "@argument(\"S17\", type=float, description=\"Value of input sensor S17\")\n",
    "@argument(\"S18\", type=float, description=\"Value of input sensor S18\")\n",
    "@argument(\"S19\", type=float, description=\"Value of input sensor S19\")\n",
    "@argument(\"S20\", type=float, description=\"Value of input sensor S20\")\n",
    "@argument(\"S21\", type=float, description=\"Value of input sensor S21\")\n",
    "@argument(\"S22\", type=float, description=\"Value of input sensor S22\")\n",
    "@argument(\"S23\", type=float, description=\"Value of input sensor S23\")\n",
    "@argument(\"S24\", type=float, description=\"Value of input sensor S24\")\n",
    "@argument(\"S25\", type=float, description=\"Value of input sensor S25\")\n",
    "@argument(\"S26\", type=float, description=\"Value of input sensor S26\")\n",
    "@argument(\"S27\", type=float, description=\"Value of input sensor S27\")\n",
    "@argument(\"S28\", type=float, description=\"Value of input sensor S28\")\n",
    "@returns(\"predicted_class\", type=float,\n",
    "         description=\"Prediction as a float: 1.0 if anomaly, 0.0 otherwise\")\n",
    "def stream_prediction(\n",
    "    Vol,\n",
    "    S1,  S2,  S3,  S4,  S5,  S6,  S7,  S8,  S9,  S10,\n",
    "    S11, S12, S13, S14, S15, S16, S17, S18, S19, S20,\n",
    "    S21, S22, S23, S24, S25, S26, S27, S28\n",
    "):\n",
    "    \"\"\"\n",
    "    This endpoint is used to connect a live stream of data to a model and\n",
    "    return predictions in real time for each new feature row as it arrives\n",
    "    from a remote location.\n",
    "    \"\"\"\n",
    "    # take the inputs and put them into and ordered np.array shape (, 29)\n",
    "    feature_list = np.array([\n",
    "        Vol,\n",
    "        S1,  S2,  S3,  S4,  S5,  S6,  S7,  S8,  S9,  S10,\n",
    "        S11, S12, S13, S14, S15, S16, S17, S18, S19, S20,\n",
    "        S21, S22, S23, S24, S25, S26, S27, S28\n",
    "    ])\n",
    "    # return the predicted output from the function row_prediction\n",
    "    return float(row_prediction(feature_list))\n",
    "\n",
    "\n",
    "@endpoint()\n",
    "@argument(\"sas_token\", type=str,\n",
    "          description=\"To score your model with the hidden data enter the \"\n",
    "                      \"sas_token provided by workshop conveners, not needed \"\n",
    "                      \"for local test\", default=\"\")\n",
    "@returns(\"score\", type=float, description=\"The score of the deployed model \"\n",
    "                                           \"on the hidden data\")\n",
    "@visualization(tab='confusion_matrix.html')\n",
    "def calculate_score(sas_token):\n",
    "    \"\"\"\n",
    "    This endpoint is used to load some hidden batch data from the cloud and\n",
    "    calculate the score of the deployed model. It will also display the\n",
    "    confusion matrix of the model in a tab on the model execution page.\n",
    "    \"\"\"\n",
    "    # get the input data to calculate the score\n",
    "    X, y = get_data(sas_token)\n",
    "    # get the prediction vector from the function vectorized_prediction\n",
    "    prediction = vectorized_prediction(X)\n",
    "    # convert the predicted data to the same format as the hidden targets\n",
    "    y_pred = ['normal' if not ipred else 'fraud' for ipred in prediction]\n",
    "    # plot and save the confusion matrix for display in the model page\n",
    "    plot_confmat(y, y_pred)\n",
    "    # return the model's score and the name of the saved plot to render\n",
    "    return get_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Before Deployment\n",
    "\n",
    "To test if `app.py` is working as expected we can run the model in the current environment before deploying. Note that if this was on your laptop you could also access the auto rendered web form by navigating to port 5000 on the local host. The VM has not been setup to render the forms so we will instead check by sending a request from the command line to that localhost port.\n",
    "\n",
    "First you will need to open a terminal from the notebook homepage and issue the command `composer run`, if this fails you will need to debug `app.py`. Otherwise you should be able to execute the two following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"predicted_class\": 0.0\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -d '@test_row.json' -H \"Content-Type: application/json\" http://localhost:5000/stream_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"score\": 0.0\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -d '{\"sas_token\": \"\"}' -H \"Content-Type: application/json\" http://localhost:5000/calculate_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying to Fabric\n",
    "\n",
    "Deployment is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model...\u001b[0m\n",
      "{'model_name': 'AutoEncoder-drillxgit', 'model_version': '0.0.1', 'wrapper_version': 2}\n",
      "\u001b[32mModel validated!\u001b[0m\n",
      "Bundling model...\u001b[0m\n",
      "Bundling app in folder /home/jovyan/work\n",
      "Making zip file: work.zip\n",
      "\u001b[32mModel bundled!\u001b[0m\n",
      "{'model_name': 'AutoEncoder-drillxgit', 'model_version': '0.0.1', 'wrapper_version': 2}\n",
      "Creating model doc...\u001b[0m\n",
      "\u001b[32mModel doc created id = 3dcf0eed-312d-d09a-1a3f-466370b08520\u001b[0m\n",
      "Uploading model zip file...\u001b[0m\n",
      "\u001b[32mModel upload succeeded\u001b[0m\n",
      "\u001b[32mBuild triggered\u001b[0m\n",
      "Build status is: queued after 1m19s \n",
      "Build status is: building after 3m52s \n",
      "Build status is: uploading after 4m31s \n",
      "Build status is: deploying after 5m51s \n",
      "Build status is: completed after 5m56s \n",
      "Model ID:         3dcf0eed-312d-d09a-1a3f-466370b08520\n",
      "Name:             AutoEncoder-drillxgit\n",
      "Version:          0.0.1\n",
      "Wrapper Version:  2\n",
      "Build Status:     completed\n",
      "Owner:            user_amld@drillx.com\n",
      "Created:          2018-01-26 22:35:57.712000\n",
      "Scale:            1/1\n",
      "Base URL:         https://sand01.arundo.com/model/3dcf0eed-312d-d09a-1a3f-466370b08520\n"
     ]
    }
   ],
   "source": [
    "!composer publish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the build or deploy fails, see if you can determine why or call one of the session conveners to help you debug. Otherwise you will now see the model listed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID                              Name                   Version    Owner                   W.V.  Build Status\r\n",
      "------------------------------------  ---------------------  ---------  --------------------  ------  --------------\r\n",
      "3dcf0eed-312d-d09a-1a3f-466370b08520  AutoEncoder-drillxgit  0.0.1      user_amld@drillx.com       2  completed\r\n",
      "f4c3b095-4f69-e088-ba65-7a5242e1add2  AutoEncoder_drillxgit  0.0.1      user_amld@drillx.com       2  completed\r\n"
     ]
    }
   ],
   "source": [
    "!composer model list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further information about the deployed model can be obtained directly from composer, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID:         3dcf0eed-312d-d09a-1a3f-466370b08520\r\n",
      "Name:             AutoEncoder-drillxgit\r\n",
      "Version:          0.0.1\r\n",
      "Wrapper Version:  2\r\n",
      "Build Status:     completed\r\n",
      "Owner:            user_amld@drillx.com\r\n",
      "Created:          2018-01-26 22:35:57.712000\r\n",
      "Scale:            1/1\r\n",
      "Base URL:         https://sand01.arundo.com/model/3dcf0eed-312d-d09a-1a3f-466370b08520\r\n"
     ]
    }
   ],
   "source": [
    "!composer model details <MODEL_UUID>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Endpoints\r\n",
      "  ├── calculate_score\r\n",
      "  │   ├── Inputs\r\n",
      "  │   │   └── sas_token: <class 'str'>\r\n",
      "  │   └── Outputs\r\n",
      "  │       └── score: <class 'float'>\r\n",
      "  └── stream_prediction\r\n",
      "      ├── Inputs\r\n",
      "      │   ├── S1: <class 'float'>\r\n",
      "      │   ├── S10: <class 'float'>\r\n",
      "      │   ├── S11: <class 'float'>\r\n",
      "      │   ├── S12: <class 'float'>\r\n",
      "      │   ├── S13: <class 'float'>\r\n",
      "      │   ├── S14: <class 'float'>\r\n",
      "      │   ├── S15: <class 'float'>\r\n",
      "      │   ├── S16: <class 'float'>\r\n",
      "      │   ├── S17: <class 'float'>\r\n",
      "      │   ├── S18: <class 'float'>\r\n",
      "      │   ├── S19: <class 'float'>\r\n",
      "      │   ├── S2: <class 'float'>\r\n",
      "      │   ├── S20: <class 'float'>\r\n",
      "      │   ├── S21: <class 'float'>\r\n",
      "      │   ├── S22: <class 'float'>\r\n",
      "      │   ├── S23: <class 'float'>\r\n",
      "      │   ├── S24: <class 'float'>\r\n",
      "      │   ├── S25: <class 'float'>\r\n",
      "      │   ├── S26: <class 'float'>\r\n",
      "      │   ├── S27: <class 'float'>\r\n",
      "      │   ├── S28: <class 'float'>\r\n",
      "      │   ├── S3: <class 'float'>\r\n",
      "      │   ├── S4: <class 'float'>\r\n",
      "      │   ├── S5: <class 'float'>\r\n",
      "      │   ├── S6: <class 'float'>\r\n",
      "      │   ├── S7: <class 'float'>\r\n",
      "      │   ├── S8: <class 'float'>\r\n",
      "      │   ├── S9: <class 'float'>\r\n",
      "      │   └── Vol: <class 'float'>\r\n",
      "      └── Outputs\r\n",
      "          └── predicted_class: <class 'float'>\r\n"
     ]
    }
   ],
   "source": [
    "!composer model endpoints <MODEL_UUID>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Streaming Pipeline\n",
    "\n",
    "Next we would like to create a pipeline to expose the data streaming into the tenant and then deploy the `stream_prediction` endpoint to that pipeline. That is very straight forward, first create the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully created pipeline (id=7f52c8b2-eed4-3412-933a-2f7e57e2d9fc).\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline create -n autoencoder_drillx_git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ID                           Pipeline Name           Owner                 Created Date\r\n",
      "------------------------------------  ----------------------  --------------------  --------------------------\r\n",
      "00d1ccfe-7b0b-e41e-c7c3-d3f752910488  autoencoder_drillxgit   user_amld@drillx.com  2018-01-25 22:35:46.590000\r\n",
      "7f52c8b2-eed4-3412-933a-2f7e57e2d9fc  autoencoder_drillx_git  user_amld@drillx.com  2018-01-26 22:47:17.545000\r\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           +---------------+\r\n",
      "---------- | Pipeline Info | ----------\r\n",
      "           +---------------+\r\n",
      "Pipeline ID:    7f52c8b2-eed4-3412-933a-2f7e57e2d9fc\r\n",
      "Pipeline Name:  autoencoder_drillx_git\r\n",
      "Created By:     user_amld@drillx.com\r\n",
      "Created Date:   2018-01-26 22:47:17.545000\r\n",
      "\r\n",
      "           +---------------+\r\n",
      "---------- | Model Details | ----------\r\n",
      "           +---------------+\r\n",
      "           +------------------+\r\n",
      "---------- | Pipeline Diagram | ----------\r\n",
      "           +------------------+\r\n",
      "  autoencoder_drillx_git\r\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline details <PIPELINE_UUID>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Model to a Pipeline\n",
    "\n",
    "Now we add the required model endpoint. Note that because we have a naming clash between the incoming sensors and the inputs defined in the endpoint we need to define a file `mapping.yaml` which defines that mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapping.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapping.yaml\n",
    "Vol: demo.fraud_amld.live_data_raw.Vol\n",
    "S1: demo.fraud_amld.live_data_raw.S1\n",
    "S2: demo.fraud_amld.live_data_raw.S2\n",
    "S3: demo.fraud_amld.live_data_raw.S3\n",
    "S4: demo.fraud_amld.live_data_raw.S4\n",
    "S5: demo.fraud_amld.live_data_raw.S5\n",
    "S6: demo.fraud_amld.live_data_raw.S6\n",
    "S7: demo.fraud_amld.live_data_raw.S7\n",
    "S8: demo.fraud_amld.live_data_raw.S8\n",
    "S9: demo.fraud_amld.live_data_raw.S9\n",
    "S10: demo.fraud_amld.live_data_raw.S10\n",
    "S11: demo.fraud_amld.live_data_raw.S11\n",
    "S12: demo.fraud_amld.live_data_raw.S12\n",
    "S13: demo.fraud_amld.live_data_raw.S13\n",
    "S14: demo.fraud_amld.live_data_raw.S14\n",
    "S15: demo.fraud_amld.live_data_raw.S15\n",
    "S16: demo.fraud_amld.live_data_raw.S16\n",
    "S17: demo.fraud_amld.live_data_raw.S17\n",
    "S18: demo.fraud_amld.live_data_raw.S18\n",
    "S19: demo.fraud_amld.live_data_raw.S19\n",
    "S20: demo.fraud_amld.live_data_raw.S20\n",
    "S21: demo.fraud_amld.live_data_raw.S21\n",
    "S22: demo.fraud_amld.live_data_raw.S22\n",
    "S23: demo.fraud_amld.live_data_raw.S23\n",
    "S24: demo.fraud_amld.live_data_raw.S24\n",
    "S25: demo.fraud_amld.live_data_raw.S25\n",
    "S26: demo.fraud_amld.live_data_raw.S26\n",
    "S27: demo.fraud_amld.live_data_raw.S27\n",
    "S28: demo.fraud_amld.live_data_raw.S28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint 'stream_prediction' input/output name map:\n",
      "\n",
      "| Group   | Model Item      | Pipeline Mapped Item                          |\n",
      "|:--------|:----------------|:----------------------------------------------|\n",
      "| input   | S1              | demo.fraud_amld.live_data_raw.S1              |\n",
      "| input   | S10             | demo.fraud_amld.live_data_raw.S10             |\n",
      "| input   | S11             | demo.fraud_amld.live_data_raw.S11             |\n",
      "| input   | S12             | demo.fraud_amld.live_data_raw.S12             |\n",
      "| input   | S13             | demo.fraud_amld.live_data_raw.S13             |\n",
      "| input   | S14             | demo.fraud_amld.live_data_raw.S14             |\n",
      "| input   | S15             | demo.fraud_amld.live_data_raw.S15             |\n",
      "| input   | S16             | demo.fraud_amld.live_data_raw.S16             |\n",
      "| input   | S17             | demo.fraud_amld.live_data_raw.S17             |\n",
      "| input   | S18             | demo.fraud_amld.live_data_raw.S18             |\n",
      "| input   | S19             | demo.fraud_amld.live_data_raw.S19             |\n",
      "| input   | S2              | demo.fraud_amld.live_data_raw.S2              |\n",
      "| input   | S20             | demo.fraud_amld.live_data_raw.S20             |\n",
      "| input   | S21             | demo.fraud_amld.live_data_raw.S21             |\n",
      "| input   | S22             | demo.fraud_amld.live_data_raw.S22             |\n",
      "| input   | S23             | demo.fraud_amld.live_data_raw.S23             |\n",
      "| input   | S24             | demo.fraud_amld.live_data_raw.S24             |\n",
      "| input   | S25             | demo.fraud_amld.live_data_raw.S25             |\n",
      "| input   | S26             | demo.fraud_amld.live_data_raw.S26             |\n",
      "| input   | S27             | demo.fraud_amld.live_data_raw.S27             |\n",
      "| input   | S28             | demo.fraud_amld.live_data_raw.S28             |\n",
      "| input   | S3              | demo.fraud_amld.live_data_raw.S3              |\n",
      "| input   | S4              | demo.fraud_amld.live_data_raw.S4              |\n",
      "| input   | S5              | demo.fraud_amld.live_data_raw.S5              |\n",
      "| input   | S6              | demo.fraud_amld.live_data_raw.S6              |\n",
      "| input   | S7              | demo.fraud_amld.live_data_raw.S7              |\n",
      "| input   | S8              | demo.fraud_amld.live_data_raw.S8              |\n",
      "| input   | S9              | demo.fraud_amld.live_data_raw.S9              |\n",
      "| input   | Vol             | demo.fraud_amld.live_data_raw.Vol             |\n",
      "| output  | predicted_class | AutoEncoder-drillxgit-0.0.1-stream_prediction |\n",
      "\n",
      "\n",
      "\u001b[32mSuccessfully added endpoint (stream_prediction) in model (id=3dcf0eed-312d-d09a-1a3f-466370b08520) to pipeline (id=7f52c8b2-eed4-3412-933a-2f7e57e2d9fc).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline add_model -e stream_prediction -mf mapping.yaml <PIPELINE_UUID> <MODEL_UUID>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           +---------------+\n",
      "---------- | Pipeline Info | ----------\n",
      "           +---------------+\n",
      "Pipeline ID:    7f52c8b2-eed4-3412-933a-2f7e57e2d9fc\n",
      "Pipeline Name:  autoencoder_drillx_git\n",
      "Created By:     user_amld@drillx.com\n",
      "Created Date:   2018-01-26 22:47:17.545000\n",
      "\n",
      "           +---------------+\n",
      "---------- | Model Details | ----------\n",
      "           +---------------+\n",
      "Model ID:      3dcf0eed-312d-d09a-1a3f-466370b08520\n",
      "Name:          AutoEncoder-drillxgit-0.0.1-stream_prediction\n",
      "Version:       0.3\n",
      "Url:           https://sand01.arundo.com/model/3dcf0eed-312d-d09a-1a3f-466370b08520/stream_prediction\n",
      "Inputs:        ['S1', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S2', 'S20', 'S21', 'S22', 'S23', 'S24', 'S25', 'S26', 'S27', 'S28', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'Vol']\n",
      "Outputs:       ['predicted_class']\n",
      "Created By:    user_amld@drillx.com\n",
      "Created Date:  2018-01-26 22:41:56.290000\n",
      "\n",
      "           +------------------+\n",
      "---------- | Pipeline Diagram | ----------\n",
      "           +------------------+\n",
      "  autoencoder_drillx_git\n",
      "  └── AutoEncoder-drillxgit-0.0.1-stream_prediction\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline details <PIPELINE_UUID>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can delete the pipeline and model we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully deleted pipeline (id=7f52c8b2-eed4-3412-933a-2f7e57e2d9fc).\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline delete <PIPELINE_UUID>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ID                           Pipeline Name          Owner                 Created Date\r\n",
      "------------------------------------  ---------------------  --------------------  --------------------------\r\n",
      "00d1ccfe-7b0b-e41e-c7c3-d3f752910488  autoencoder_drillxgit  user_amld@drillx.com  2018-01-25 22:35:46.590000\r\n"
     ]
    }
   ],
   "source": [
    "!composer pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model 3dcf0eed-312d-d09a-1a3f-466370b08520\n",
      "\u001b[32mDeleted your model 3dcf0eed-312d-d09a-1a3f-466370b08520\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!composer model delete <MODEL_UUID>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID                              Name                   Version    Owner                   W.V.  Build Status\r\n",
      "------------------------------------  ---------------------  ---------  --------------------  ------  --------------\r\n",
      "f4c3b095-4f69-e088-ba65-7a5242e1add2  AutoEncoder_drillxgit  0.0.1      user_amld@drillx.com       2  completed\r\n"
     ]
    }
   ],
   "source": [
    "!composer model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
